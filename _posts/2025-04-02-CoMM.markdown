---
layout:     post
title:      "CoMM"
subtitle:   "A Coherent Interleaved Image-Text Dataset for Multimodal Understanding and Generation"
arxiv:      "https://arxiv.org/pdf/2406.10462"
date:       2025-04-02
author:     "Wei Chen, Lin Li, Yongqi Yang, Bin Wen, Fan Yang, Tingting Gao, Yu Wu, Long Chen"
img: "img/CoMM.jpg"
---

Interleaved image-text generation is a vital multimodal task, but generating sequences with narrative coherence, entity/style consistency, and alignment remains challenging due to poor training data quality. To address this, we introduce **CoMM**, a high-quality **Coherent interleaved image-text MultiModal dataset** designed to enhance coherence, consistency, and alignment. CoMM harnesses raw data from diverse sources (instructional content and visual storytelling) as a foundation, then applies a **multi-perspective filter strategy** using advanced pre-trained models to ensure sentence development, image consistency, and semantic alignment between images and text. Quality evaluation metrics confirm the dataset's high quality, while extensive few-shot experiments demonstrate CoMM's effectiveness in enhancing multimodal large language models' (MLLMs) in-context learning capabilities. We also propose **four new tasks** and a comprehensive evaluation framework to assess interleaved generation abilities, opening new avenues for advanced MLLMs with superior multimodal understanding.
