---
layout:     post
title:      "STGC"
subtitle:   "SOLVING TOKEN GRADIENT CONFLICT IN MIXTUREOF-EXPERTS FOR LARGE VISION-LANGUAGE MODEL"
date:       2025-03-15
author:     "Longrong Yang, Dong Shen, Chaoxiang Cai, Fan Yang, Tingting Gao, Di Zhang, Xi Li"
img: "img/STGC.jpg"
---

The Mixture-of-Experts (MoE) approach in Large Vision-Language Models (LVLMs) uses sparse models to achieve comparable performance with fewer activated parameters during inference, reducing costs. However, existing MoE methods employ routers that are not optimized for distinct parameter optimization directions from tokens within an expert, causing severe interference between tokens. To address this, we propose **Solving Token Gradient Conflict (STGC)**, which utilizes token-level gradient analysis to identify conflicting tokens in experts and adds a tailored regularization loss to encourage routing conflicting tokens away from their current experts, thereby reducing intra-expert interference. STGC serves as a plug-in for diverse LVLM methods, with extensive experiments demonstrating its effectiveness. The code will be available at [https://github.com/longrongyang/STGC](https://github.com/longrongyang/STGC).
